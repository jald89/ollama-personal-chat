# ðŸ¤– TechBot - Chat Interactivo con Ollama y Llama 3.2

> **Â¿Buscas la documentaciÃ³n tÃ©cnica y explicaciÃ³n de la lÃ³gica? Consulta [`docs.md`](./docs.md)**

## ðŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa un **chat interactivo con memoria** utilizando **Ollama** y el modelo **Llama 3.2**. Incluye un **agente personalizado** (TechBot) especializado en soporte tÃ©cnico con capacidad de mantener contexto a lo largo de conversaciones completas.

## âœ¨ CaracterÃ­sticas Implementadas

### ðŸ§  Memoria de ConversaciÃ³n
- **Persistencia de contexto**: El chat mantiene historial completo de la conversaciÃ³n
- **Memoria dinÃ¡mica**: Cada sesiÃ³n mantiene su propio contexto independiente
- **GestiÃ³n inteligente**: Limita automÃ¡ticamente el historial para optimizar rendimiento

### ðŸ¤– Agente Personalizado - TechBot
- **Personalidad definida**: Asistente de soporte tÃ©cnico amigable y profesional
- **EspecializaciÃ³n**: Enfocado en programaciÃ³n y tecnologÃ­a
- **Comportamiento consistente**: Mantiene su rol a lo largo de toda la conversaciÃ³n
- **Respuestas estructuradas**: Explica conceptos paso a paso con ejemplos prÃ¡cticos

### ðŸ–¥ï¸ Interfaz de Usuario
- **Frontend Streamlit**: Interfaz web moderna y reactiva
- **Backend Flask**: API REST robusta para gestiÃ³n de conversaciones
- **GestiÃ³n de sesiones**: MÃºltiples conversaciones independientes
- **Indicadores visuales**: Estados de conexiÃ³n y contadores de mensajes

## ðŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP/JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Python SDK    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚                 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚                 â”‚
â”‚   Streamlit     â”‚                 â”‚   Flask API     â”‚                  â”‚     Ollama      â”‚
â”‚   (Frontend)    â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Backend)     â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Llama 3.2)   â”‚
â”‚                 â”‚    Responses    â”‚                 â”‚    Responses     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos:
1. **Usuario** envÃ­a mensaje en Streamlit
2. **Streamlit** hace POST a Flask API
3. **Flask** recupera historial de conversaciÃ³n (memoria)
4. **Flask** envÃ­a contexto completo a Ollama
5. **Ollama** procesa con Llama 3.2 y responde
6. **Flask** guarda respuesta en memoria y la retorna
7. **Streamlit** muestra respuesta al usuario

## ðŸ“ Estructura del Proyecto

```
proyecto/
â”œâ”€â”€ app.py                 # Servidor Flask con memoria y agente
â”œâ”€â”€ streamlit_app.py       # Interfaz Streamlit
â”œâ”€â”€ test_memory.py         # Script de pruebas automatizadas
â”œâ”€â”€ requirements.txt       # Dependencias Python
â””â”€â”€ README.md             # DocumentaciÃ³n
```

## ðŸš€ InstalaciÃ³n y EjecuciÃ³n

### Prerequisitos

- Ollama instalado: Descargar de [ollama.com](https://ollama.com)
- Modelo Llama 3.2: `ollama pull llama3.2:1b`
- Python 3.8+

### Pasos de instalaciÃ³n:

```bash
# 1. Clonar o descargar los archivos del proyecto
cd single_agent
pip install -r requirements.txt
```

### OpciÃ³n 1: Inicio RÃ¡pido

```bash
# Desde la raÃ­z del monorepo
./start.sh single_agent
```

### OpciÃ³n 2: Inicio Manual

```bash
# Terminal 1 - Ejecutar servidor Flask
python app.py
# o
flask run --port=5050

# Terminal 2 - Ejecutar Streamlit
streamlit run web.py
```

### URLs de acceso:

- **Chat Interface**: http://localhost:8501
- **Flask API**: http://127.0.0.1:5050

## ðŸ§ª Pruebas de Funcionalidad

### Pruebas Manuales
1. **Memoria**: Haz preguntas de seguimiento que dependan de respuestas anteriores
2. **Agente**: Observa la personalidad consistente de TechBot
3. **Contexto**: Verifica que recuerda informaciÃ³n de mensajes previos

### Pruebas Automatizadas
Ejecuta `python test_memory.py` para probar:
- âœ… Persistencia de memoria
- âœ… Personalidad del agente
- âœ… Contexto de conversaciÃ³n
- âœ… Conectividad con Ollama

### Ejemplo de ConversaciÃ³n con Memoria:

```
Usuario: "Hola, necesito ayuda con Python"
TechBot: "Â¡Hola! Soy TechBot... Â¿QuÃ© aspecto especÃ­fico de Python te gustarÃ­a aprender?"

Usuario: "Â¿QuÃ© son las listas?"
TechBot: "Las listas en Python son estructuras de datos... [explicaciÃ³n detallada]"

Usuario: "Â¿Puedes darme un ejemplo de lo que acabas de explicar?"
TechBot: "Â¡Por supuesto! BasÃ¡ndome en la explicaciÃ³n sobre listas que te di..."
                    â†‘ (Demuestra memoria del contexto anterior)
```

## ðŸ”§ ConfiguraciÃ³n del Agente

### Personalidad Actual (TechBot):
- **Rol**: Asistente de Soporte TÃ©cnico
- **EspecializaciÃ³n**: ProgramaciÃ³n y tecnologÃ­a
- **Tono**: Amigable, profesional, paciente
- **Estilo**: Explicaciones paso a paso con ejemplos

### PersonalizaciÃ³n:
Para cambiar la personalidad del agente, modifica `AGENT_SYSTEM_MESSAGE` en `app.py`:

```python
AGENT_SYSTEM_MESSAGE = {
    'role': 'system',
    'content': 'Tu nueva personalidad aquÃ­...'
}
```

## ðŸŽ¯ Cumplimiento de Requisitos

### âœ… Requisitos Cumplidos:

1. **Chat interactivo con Ollama y Llama 3.2** âœ…
   - Implementado con Flask + Streamlit
   - Usa modelo llama3.2:1b

2. **Memoria para mantener contexto** âœ…
   - Sistema de historial persistente
   - Contexto completo en cada llamada
   - GestiÃ³n inteligente de memoria

3. **Agente personalizado** âœ…
   - TechBot con personalidad definida
   - Comportamiento consistente
   - EspecializaciÃ³n en soporte tÃ©cnico

4. **InvestigaciÃ³n de documentaciÃ³n** âœ…
   - ImplementaciÃ³n basada en mejores prÃ¡cticas de Ollama
   - Uso correcto de mensajes con roles (system, user, assistant)

5. **Scripts Python funcionales** âœ…
   - Backend Flask robusto
   - Frontend Streamlit interactivo
   - Script de pruebas automatizadas

## ðŸ“Š MÃ©tricas de Rendimiento

- **Tiempo de respuesta**: ~2-5 segundos (depende del hardware)
- **GestiÃ³n de memoria**: Limitado a Ãºltimos 20 mensajes + contexto de sistema
- **Sesiones concurrentes**: Soporta mÃºltiples usuarios independientes
- **TamaÃ±o de contexto**: Optimizado para evitar lÃ­mites de tokens

## ðŸ” Troubleshooting

### Problemas Comunes:

1. **"model not found"**
   ```bash
   ollama pull llama3.2:1b
   ```

2. **"Connection refused"**
   - Verificar que Ollama estÃ© corriendo
   - Verificar que Flask estÃ© en puerto 5000

3. **Respuestas lentas**
   - Normal con modelos locales
   - Considera usar modelo mÃ¡s pequeÃ±o

## ðŸŽ“ Conceptos TÃ©cnicos Implementados

### Memoria de ConversaciÃ³n:
- **Persistencia**: Almacenamiento en memoria del servidor
- **Contexto completo**: Cada llamada incluye historial completo
- **GestiÃ³n de tokens**: Limita historial para evitar overflow

### Agente con Personalidad:
- **System Message**: Define comportamiento del agente
- **Consistencia**: Mantiene personalidad a lo largo de conversaciÃ³n
- **EspecializaciÃ³n**: Enfoque especÃ­fico en soporte tÃ©cnico

### Arquitectura:
- **SeparaciÃ³n de responsabilidades**: Frontend/Backend separados
- **API RESTful**: Endpoints bien definidos
- **GestiÃ³n de estado**: Sesiones independientes por usuario

## ðŸ† ConclusiÃ³n

Este proyecto demuestra una implementaciÃ³n completa de un chat inteligente con:
- âœ… **Memoria funcional** que mantiene contexto
- âœ… **Agente personalizado** con comportamiento consistente  
- âœ… **Interfaz profesional** fÃ¡cil de usar
- âœ… **Arquitectura escalable** y bien estructurada
- âœ… **Pruebas automatizadas** para validar funcionalidad

**Todas las especificaciones de la tarea han sido implementadas exitosamente.**