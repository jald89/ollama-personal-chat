ðŸ¤– TechBot - Chat Interactivo con Ollama y Llama 3.2
ðŸ“‹ DescripciÃ³n del Proyecto
Este proyecto implementa un chat interactivo con memoria utilizando Ollama y el modelo Llama 3.2. Incluye un agente personalizado (TechBot) especializado en soporte tÃ©cnico con capacidad de mantener contexto a lo largo de conversaciones completas.
âœ¨ CaracterÃ­sticas Implementadas
ðŸ§  Memoria de ConversaciÃ³n

Persistencia de contexto: El chat mantiene historial completo de la conversaciÃ³n
Memoria dinÃ¡mica: Cada sesiÃ³n mantiene su propio contexto independiente
GestiÃ³n inteligente: Limita automÃ¡ticamente el historial para optimizar rendimiento

ðŸ¤– Agente Personalizado - TechBot

Personalidad definida: Asistente de soporte tÃ©cnico amigable y profesional
EspecializaciÃ³n: Enfocado en programaciÃ³n y tecnologÃ­a
Comportamiento consistente: Mantiene su rol a lo largo de toda la conversaciÃ³n
Respuestas estructuradas: Explica conceptos paso a paso con ejemplos prÃ¡cticos

ðŸ–¥ï¸ Interfaz de Usuario

Frontend Streamlit: Interfaz web moderna y reactiva
Backend Flask: API REST robusta para gestiÃ³n de conversaciones
GestiÃ³n de sesiones: MÃºltiples conversaciones independientes
Indicadores visuales: Estados de conexiÃ³n y contadores de mensajes

ðŸ—ï¸ Arquitectura del Sistema
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP/JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Python SDK    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚                 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚                 â”‚
â”‚   Streamlit     â”‚                 â”‚   Flask API     â”‚                  â”‚     Ollama      â”‚
â”‚   (Frontend)    â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Backend)     â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Llama 3.2)   â”‚
â”‚                 â”‚    Responses    â”‚                 â”‚    Responses     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Flujo de Datos:

Usuario envÃ­a mensaje en Streamlit
Streamlit hace POST a Flask API
Flask recupera historial de conversaciÃ³n (memoria)
Flask envÃ­a contexto completo a Ollama
Ollama procesa con Llama 3.2 y responde
Flask guarda respuesta en memoria y la retorna
Streamlit muestra respuesta al usuario

ðŸ“ Estructura del Proyecto
proyecto/
â”œâ”€â”€ app.py                 # Servidor Flask con memoria y agente
â”œâ”€â”€ streamlit_app.py       # Interfaz Streamlit
â”œâ”€â”€ test_memory.py         # Script de pruebas automatizadas
â”œâ”€â”€ requirements.txt       # Dependencias Python
â””â”€â”€ README.md             # DocumentaciÃ³n
ðŸš€ InstalaciÃ³n y EjecuciÃ³n
Prerequisitos

Ollama instalado: Descargar de ollama.com
Modelo Llama 3.2: ollama pull llama3.2:1b
Python 3.8+

Pasos de instalaciÃ³n:
bash# 1. Clonar o descargar los archivos del proyecto
cd tu-directorio-proyecto

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Verificar que Ollama estÃ© corriendo
ollama list  # Debe mostrar llama3.2:1b

# 4. Ejecutar servidor Flask (Terminal 1)
python app.py

# 5. Ejecutar Streamlit (Terminal 2)
streamlit run streamlit_app.py

# 6. (Opcional) Ejecutar pruebas automatizadas (Terminal 3)
python test_memory.py
URLs de acceso:

Chat Interface: http://localhost:8501
Flask API: http://127.0.0.1:5000
API Endpoints:

POST /chat - Enviar mensaje
POST /reset - Reiniciar conversaciÃ³n
GET /history - Obtener historial



ðŸ§ª Pruebas de Funcionalidad
Pruebas Manuales

Memoria: Haz preguntas de seguimiento que dependan de respuestas anteriores
Agente: Observa la personalidad consistente de TechBot
Contexto: Verifica que recuerda informaciÃ³n de mensajes previos

Pruebas Automatizadas
Ejecuta python test_memory.py para probar:

âœ… Persistencia de memoria
âœ… Personalidad del agente
âœ… Contexto de conversaciÃ³n
âœ… Conectividad con Ollama

Ejemplo de ConversaciÃ³n con Memoria:
Usuario: "Hola, necesito ayuda con Python"
TechBot: "Â¡Hola! Soy TechBot... Â¿QuÃ© aspecto especÃ­fico de Python te gustarÃ­a aprender?"

Usuario: "Â¿QuÃ© son las listas?"
TechBot: "Las listas en Python son estructuras de datos... [explicaciÃ³n detallada]"

Usuario: "Â¿Puedes darme un ejemplo de lo que acabas de explicar?"
TechBot: "Â¡Por supuesto! BasÃ¡ndome en la explicaciÃ³n sobre listas que te di..."
                    â†‘ (Demuestra memoria del contexto anterior)
ðŸ”§ ConfiguraciÃ³n del Agente
Personalidad Actual (TechBot):

Rol: Asistente de Soporte TÃ©cnico
EspecializaciÃ³n: ProgramaciÃ³n y tecnologÃ­a
Tono: Amigable, profesional, paciente
Estilo: Explicaciones paso a paso con ejemplos

PersonalizaciÃ³n:
Para cambiar la personalidad del agente, modifica AGENT_SYSTEM_MESSAGE en app.py:
pythonAGENT_SYSTEM_MESSAGE = {
    'role': 'system',
    'content': 'Tu nueva personalidad aquÃ­...'
}
ðŸŽ¯ Cumplimiento de Requisitos
âœ… Requisitos Cumplidos:

Chat interactivo con Ollama y Llama 3.2 âœ…

Implementado con Flask + Streamlit
Usa modelo llama3.2:1b


Memoria para mantener contexto âœ…

Sistema de historial persistente
Contexto completo en cada llamada
GestiÃ³n inteligente de memoria


Agente personalizado âœ…

TechBot con personalidad definida
Comportamiento consistente
EspecializaciÃ³n en soporte tÃ©cnico


InvestigaciÃ³n de documentaciÃ³n âœ…

ImplementaciÃ³n basada en mejores prÃ¡cticas de Ollama
Uso correcto de mensajes con roles (system, user, assistant)


Scripts Python funcionales âœ…

Backend Flask robusto
Frontend Streamlit interactivo
Script de pruebas automatizadas



ðŸ“Š MÃ©tricas de Rendimiento

Tiempo de respuesta: ~2-5 segundos (depende del hardware)
GestiÃ³n de memoria: Limitado a Ãºltimos 20 mensajes + contexto de sistema
Sesiones concurrentes: Soporta mÃºltiples usuarios independientes
TamaÃ±o de contexto: Optimizado para evitar lÃ­mites de tokens

ðŸ” Troubleshooting
Problemas Comunes:

"model not found"
bashollama pull llama3.2:1b

"Connection refused"

Verificar que Ollama estÃ© corriendo
Verificar que Flask estÃ© en puerto 5000


Respuestas lentas

Normal con modelos locales
Considera usar modelo mÃ¡s pequeÃ±o



ðŸŽ“ Conceptos TÃ©cnicos Implementados
Memoria de ConversaciÃ³n:

Persistencia: Almacenamiento en memoria del servidor
Contexto completo: Cada llamada incluye historial completo
GestiÃ³n de tokens: Limita historial para evitar overflow

Agente con Personalidad:

System Message: Define comportamiento del agente
Consistencia: Mantiene personalidad a lo largo de conversaciÃ³n
EspecializaciÃ³n: Enfoque especÃ­fico en soporte tÃ©cnico

Arquitectura:

SeparaciÃ³n de responsabilidades: Frontend/Backend separados
API RESTful: Endpoints bien definidos
GestiÃ³n de estado: Sesiones independientes por usuario

ðŸ† ConclusiÃ³n
Este proyecto demuestra una implementaciÃ³n completa de un chat inteligente con:

âœ… Memoria funcional que mantiene contexto
âœ… Agente personalizado con comportamiento consistente
âœ… Interfaz profesional fÃ¡cil de usar
âœ… Arquitectura escalable y bien estructurada
âœ… Pruebas automatizadas para validar funcionalidad

Todas las especificaciones de la tarea han sido implementadas exitosamente.